{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import optuna\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('./test.csv').drop(columns=['ID'])\n",
    "\n",
    "X_train = train.drop('ì„ì‹  ì„±ê³µ ì—¬ë¶€', axis=1)\n",
    "y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜\",\n",
    "    \"ì´ ìƒì„± ë°°ì•„ ìˆ˜\",  #used ####\n",
    "    \"ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"ì´ì‹ëœ ë°°ì•„ ìˆ˜\",   #used\n",
    "    \"ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜\",  #####\n",
    "    \"ì €ì¥ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"ë¯¸ì„¸ì£¼ì… í›„ ì €ì¥ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"í•´ë™ëœ ë°°ì•„ ìˆ˜\",\n",
    "    \"í•´ë™ ë‚œì ìˆ˜\", #used\n",
    "    \"ìˆ˜ì§‘ëœ ì‹ ì„  ë‚œì ìˆ˜\",  #used #####\n",
    "    \"ì €ì¥ëœ ì‹ ì„  ë‚œì ìˆ˜\",\n",
    "    \"í˜¼í•©ëœ ë‚œì ìˆ˜\",   #used #####\n",
    "    \"íŒŒíŠ¸ë„ˆ ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜\", #####\n",
    "    \"ê¸°ì¦ì ì •ìì™€ í˜¼í•©ëœ ë‚œì ìˆ˜\",\n",
    "    \"ë‚œì ì±„ì·¨ ê²½ê³¼ì¼\", #####\n",
    "    \"ë‚œì í•´ë™ ê²½ê³¼ì¼\",\n",
    "    \"ë‚œì í˜¼í•© ê²½ê³¼ì¼\",\n",
    "    \"ë°°ì•„ ì´ì‹ ê²½ê³¼ì¼\", #####\n",
    "    \"ë°°ì•„ í•´ë™ ê²½ê³¼ì¼\"\n",
    "]\n",
    "print(len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\",\n",
    "    \"ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´\", #used # most important factor\n",
    "    \"ì‹œìˆ  ìœ í˜•\",\n",
    "    \"íŠ¹ì • ì‹œìˆ  ìœ í˜•\",\n",
    "    \"ë°°ë€ ìê·¹ ì—¬ë¶€\",   #used -> eliminated # ì—†ì•´ë”ë‹ˆ ì„±ëŠ¥ ê°ì†Œ\n",
    "    \"ë°°ë€ ìœ ë„ ìœ í˜•\",\n",
    "    \"ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸\",    #used\n",
    "    \"ë¶ˆëª…í™• ë¶ˆì„ ì›ì¸\", #used # ì—† ê°\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\",    #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\",    #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë°°ë€ ì¥ì• \",    #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸\",    #used -> eliminated   # ì–˜ëŠ” ê·¸ëŒ€ë¡œ\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ\",    #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ë‚´ë§‰ì¦\",   #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë†ë„\",    #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë©´ì—­í•™ì  ìš”ì¸\",   #used -> eliminated # ì—†ì•´ë”ë‹ˆ ì„±ëŠ¥ ê°ì†Œ\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ìš´ë™ì„±\",  #used\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì í˜•íƒœ\",    #used\n",
    "    \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
    "    \"ì´ ì‹œìˆ  íšŸìˆ˜\", #used\n",
    "    \"í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"IVF ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"DI ì‹œìˆ  íšŸìˆ˜\",\n",
    "    \"ì´ ì„ì‹  íšŸìˆ˜\", #used\n",
    "    \"IVF ì„ì‹  íšŸìˆ˜\",\n",
    "    \"DI ì„ì‹  íšŸìˆ˜\",\n",
    "    \"ì´ ì¶œì‚° íšŸìˆ˜\", #used\n",
    "    \"IVF ì¶œì‚° íšŸìˆ˜\",\n",
    "    \"DI ì¶œì‚° íšŸìˆ˜\",\n",
    "    \"ë‚œì ì¶œì²˜\",    #used -> eliminated\n",
    "    \"ì •ì ì¶œì²˜\",    #used -> eliminated\n",
    "    \"ë‚œì ê¸°ì¦ì ë‚˜ì´\",\n",
    "    \"ì •ì ê¸°ì¦ì ë‚˜ì´\",\n",
    "    \"ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",  #used\n",
    "    \"ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",  #used\n",
    "    \"ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ëŒ€ë¦¬ëª¨ ì—¬ë¶€\",\n",
    "    \"PGD ì‹œìˆ  ì—¬ë¶€\",\n",
    "    \"PGS ì‹œìˆ  ì—¬ë¶€\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Categorical_non_ordinal=[\n",
    "    \"ì‹œìˆ  ì‹œê¸° ì½”ë“œ\",\n",
    "    \"ì‹œìˆ  ìœ í˜•\",\n",
    "    \"ë°°ë€ ìê·¹ ì—¬ë¶€\",\n",
    "    \"ë°°ë€ ìœ ë„ ìœ í˜•\",\n",
    "    \"ë‹¨ì¼ ë°°ì•„ ì´ì‹ ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ê²€ì‚¬ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶ˆëª…í™• ë¶ˆì„ ì›ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë‚¨ì„± ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ë°°ë€ ì¥ì• \",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ìê¶ë‚´ë§‰ì¦\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë†ë„\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ë©´ì—­í•™ì  ìš”ì¸\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì ìš´ë™ì„±\",\n",
    "    \"ë¶ˆì„ ì›ì¸ - ì •ì í˜•íƒœ\",\n",
    "    \"ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ \",\n",
    "    \"ë‚œì ì¶œì²˜\",\n",
    "    \"ì •ì ì¶œì²˜\",\n",
    "    \"ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€\",\n",
    "    \"ëŒ€ë¦¬ëª¨ ì—¬ë¶€\",\n",
    "    \"PGD ì‹œìˆ  ì—¬ë¶€\",\n",
    "    \"PGS ì‹œìˆ  ì—¬ë¶€\",\n",
    "    \"íŠ¹ì • ì‹œìˆ  ìœ í˜•\"\n",
    "] #34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PreProcessing0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Basic PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Data Drop & Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = ['ì‹œìˆ  ìœ í˜•', 'ì°©ìƒ ì „ ìœ ì „ ì§„ë‹¨ ì‚¬ìš© ì—¬ë¶€', 'ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸', 'ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸', 'ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸', \n",
    "              'ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸', 'ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸', 'ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸', 'ë¶ˆëª…í™• ë¶ˆì„ ì›ì¸', 'ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸', \n",
    "              'ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ', 'ë¶ˆì„ ì›ì¸ - ìê¶ë‚´ë§‰ì¦', 'ë¶ˆì„ ì›ì¸ - ì •ì ë†ë„', \n",
    "              'ë¶ˆì„ ì›ì¸ - ì •ì ìš´ë™ì„±', 'DI ì„ì‹  íšŸìˆ˜', 'DI ì¶œì‚° íšŸìˆ˜', 'ë™ê²° ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', \n",
    "              'ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', 'ê¸°ì¦ ë°°ì•„ ì‚¬ìš© ì—¬ë¶€', 'ëŒ€ë¦¬ëª¨ ì—¬ë¶€']\n",
    "\n",
    "numeric_columns = list(set(numeric_columns) - set(out_list))\n",
    "categorical_columns = list(set(categorical_columns) - set(out_list))\n",
    "Categorical_non_ordinal = list(set(Categorical_non_ordinal) - set(out_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=out_list)\n",
    "test = test.drop(columns=out_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Numeric Drop '''\n",
    "#numeric - ê²°ì¸¡ì¹˜ 80% ì´ìƒ drop\n",
    "for col in numeric_columns:\n",
    "    print(X_train[col].isnull().sum())\n",
    "numeric_null_columns = [ col for col in numeric_columns if X_train[col].isnull().sum() != 0 ]\n",
    "\n",
    "numeric_null_ratio = X_train[numeric_null_columns].isnull().mean().sort_values(ascending=False)\n",
    "print(numeric_null_ratio)\n",
    "\n",
    "drop_columns = numeric_null_ratio[numeric_null_ratio >= 0.8].index\n",
    "drop_columns = drop_columns.drop('ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜')\n",
    "X_train = X_train.drop(columns=drop_columns)\n",
    "test = test.drop(columns=drop_columns)\n",
    "\n",
    "numeric_columns = list(set(numeric_columns) - set(drop_columns))\n",
    "print(\"ğŸ“Œ\"+drop_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Numeric Impute 2 : KNN Imputer for model 2,3'''\n",
    "import joblib \n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer_cache_path = \"knn_imputed2.pkl\"\n",
    "\n",
    "try:\n",
    "    X_train_imputed, test_imputed = joblib.load(imputer_cache_path)\n",
    "    print(\"Loaded cached KNN imputed data.\")\n",
    "\n",
    "    ########################################################################\n",
    "    # X_train_imputed = X_train_imputed.drop(columns=out_list)\n",
    "    # test_imputed = test_imputed.drop(columns=out_list)\n",
    "    ########################################################################\n",
    "except FileNotFoundError:\n",
    "    numeric_filled_columns = numeric_null_ratio[numeric_null_ratio < 0.8].index  # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ 80% ë¯¸ë§Œ ì»¬ëŸ¼ ì„ íƒ\n",
    "    \n",
    "    knn_imputer = KNNImputer(n_neighbors=6)\n",
    "    \n",
    "    X_train_imputed = X_train.copy()\n",
    "    test_imputed = test.copy()\n",
    "    \n",
    "    X_train_imputed[numeric_filled_columns] = knn_imputer.fit_transform(X_train[numeric_filled_columns])\n",
    "    print(\"X_train finish\")\n",
    "    test_imputed[numeric_filled_columns] = knn_imputer.transform(test[numeric_filled_columns])\n",
    "    print(\"All columns imputed.\")\n",
    "    \n",
    "    joblib.dump((X_train_imputed, test_imputed), imputer_cache_path)\n",
    "    print(\"Saved KNN imputed data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed['ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜'] = X_train_imputed['ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜'].fillna(0)\n",
    "test_imputed['ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜'] = test_imputed['ì„ì‹  ì‹œë„ ë˜ëŠ” ë§ˆì§€ë§‰ ì„ì‹  ê²½ê³¼ ì—°ìˆ˜'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Numeric Impute only for Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Categorical Drop'''\n",
    "# ordinal / non ordinal categorical columns êµ¬ë¶„\n",
    "''' ordinal ì˜ ê²½ìš° ê²°ì¸¡ì¹˜ ë¬´ì˜ë¯¸ '''\n",
    "''' non ordinal categorical drop '''\n",
    "# ìˆœì„œê°€ ì—†ëŠ” categorical columns ì¤‘ ê²°ì¸¡ì¹˜ 90% ì´ìƒì¸ columns drop\n",
    "\n",
    "categorical_null_columns = []\n",
    "print(X_train_imputed[Categorical_non_ordinal].isnull().sum())\n",
    "categorical_null_columns = [ col for col in categorical_columns if X_train_imputed[col].isnull().sum() != 0 ]\n",
    "\n",
    "# print(categorical_null_columns)\n",
    "\n",
    "categorical_null_ratio = X_train_imputed[categorical_null_columns].isnull().mean().sort_values(ascending=False)\n",
    "print(categorical_null_ratio)\n",
    "\n",
    "drop_columns = categorical_null_ratio[categorical_null_ratio >= 0.9].index\n",
    "X_train_imputed = X_train_imputed.drop(columns=drop_columns)\n",
    "test_imputed = test_imputed.drop(columns=drop_columns)\n",
    "\n",
    "categorical_columns = list(set(categorical_columns) - set(drop_columns))\n",
    "print(f\"ë‚¨ì€ categorical_columns: {categorical_columns}\")\n",
    "print(len(categorical_columns))\n",
    "print(\"ğŸ“Œ\"+drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Categorical Impute only for Model 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Categorical Impute2 '''\n",
    "#model3\n",
    "imputer3 = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train_imputed3 = X_train_imputed.copy()\n",
    "test_imputed3 = test_imputed.copy()\n",
    "\n",
    "X_train_imputed3[categorical_columns] = imputer3.fit_transform(X_train_imputed[categorical_columns])\n",
    "test_imputed3[categorical_columns] = imputer3.transform(test_imputed[categorical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì´ìƒì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°\n",
    "# def calculate_outlier_ratio(data):\n",
    "#     # ê° ì»¬ëŸ¼ì— ëŒ€í•´ ì´ìƒì¹˜ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜\n",
    "#     Q1 = np.percentile(data, 25)\n",
    "#     Q3 = np.percentile(data, 75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "#     # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°\n",
    "#     outliers = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "#     total = len(data)\n",
    "#     return outliers / total\n",
    "\n",
    "# # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°\n",
    "# outlier_ratios = {}\n",
    "# for column in numeric_columns:\n",
    "#     outlier_ratios[column] = calculate_outlier_ratio(X_train_imputed[column])\n",
    "\n",
    "# # ì´ìƒì¹˜ ë¹„ìœ¨ ì¶œë ¥\n",
    "# for column, ratio in outlier_ratios.items():\n",
    "#     print(f\"{column}ì˜ ì´ìƒì¹˜ ë¹„ìœ¨: {ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "# print(f\"LOF ì ìš© ì „ ë°ì´í„° í¬ê¸°: {X_train_imputed3.shape}\")\n",
    "\n",
    "# lof = LocalOutlierFactor(n_neighbors=30, contamination=0.07)\n",
    "# outliers = lof.fit_predict(X_train_imputed3[numeric_columns])\n",
    "\n",
    "# X_train_imputed3 = X_train_imputed3[outliers == 1]\n",
    "# y = y[outliers == 1]\n",
    "\n",
    "# print(f\"LOF ì ìš© í›„ ë°ì´í„° í¬ê¸°: {X_train_imputed3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train_imputed3.copy()\n",
    "test_scaled = test_imputed3.copy()\n",
    "\n",
    "X_train_scaled[numeric_columns] = scaler.fit_transform(X_train_imputed3[numeric_columns])\n",
    "test_scaled[numeric_columns] = scaler.transform(test_imputed3[numeric_columns])\n",
    "print(X_train_imputed3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. PreProcessing2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric = X_train_imputed3[numeric_columns]\n",
    "test_numeric = test_imputed3[numeric_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_numeric)\n",
    "test_scaled = scaler.transform(test_numeric)\n",
    "\n",
    "# PCA ì ìš© (ì„¤ì •: ë¶„ì‚°ì„ 95% ìœ ì§€í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜ ìë™ ì„ íƒ)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "test_pca = pca.transform(test_scaled)\n",
    "\n",
    "# PCA ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "test_pca = pd.DataFrame(test_pca, columns=[f'PC{i+1}' for i in range(test_pca.shape[1])])\n",
    "\n",
    "# PCA ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ í™•ì¸\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "num_components = len(cumulative_variance)\n",
    "\n",
    "print(f\"PCA :  ì‚¬ìš©ëœ ì£¼ì„±ë¶„ ê°œìˆ˜: {num_components}\")\n",
    "print(f\"ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨ (ëˆ„ì ): {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ì—ì„œ ìˆ«ìí˜• ë¶€ë¶„ì„ PCA ê²°ê³¼ë¡œ êµì²´\n",
    "X_train_imputed3.update(X_train_pca)\n",
    "test_imputed3.update(test_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. ì„¸ì œê³± ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(set(numeric_columns)-set(['ì´ì‹ëœ ë°°ì•„ ìˆ˜'])):\n",
    "  X_train_imputed3[col] = X_train_imputed3[col] ** 2\n",
    "  test_imputed3[col] = test_imputed3[col] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Feature ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-1. ë‹¨ìˆœ ìƒì„±ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed3['ì‹œìˆ  ì‹œì '] = X_train_imputed3['ì‹œìˆ  ì‹œê¸° ì½”ë“œ'] + '_' + X_train_imputed3['ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´']\n",
    "test_imputed3['ì‹œìˆ  ì‹œì '] = test_imputed3['ì‹œìˆ  ì‹œê¸° ì½”ë“œ'] + '_' + test_imputed3['ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´']\n",
    "categorical_columns.append('ì‹œìˆ  ì‹œì ')\n",
    "Categorical_non_ordinal.append('ì‹œìˆ  ì‹œì ')\n",
    "\n",
    "# X_train_imputed3['ë°°ì•„ ì •ë³´'] = (X_train_imputed3['ì´ ìƒì„± ë°°ì•„ ìˆ˜'] * X_train_imputed3['í•´ë™ëœ ë°°ì•„ ìˆ˜'])\n",
    "# numeric_columns.append('ë°°ì•„ ì •ë³´')\n",
    "\n",
    "# print(X_train_imputed['ë°°ì•„ ì •ë³´']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-2. ê²°ì¸¡ì¹˜ ë³´ì™„ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_newf = X_train_imputed3.copy()\n",
    "test_newf = test_imputed3.copy()\n",
    "y_train3 = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' train / valid êµ¬ë¶„ '''  #70% train\n",
    "#model3\n",
    "X_train3 = X_train_newf[:int(X_train.shape[0]*0.7):]\n",
    "X_valid3 = X_train_newf[int(X_train.shape[0]*0.7):]\n",
    "y_train3 = y[:int(X_train.shape[0]*0.7):]\n",
    "y_valid3 = y[int(X_train.shape[0]*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' model 3 '''\n",
    "\n",
    "#categorical str transform\n",
    "for col in categorical_columns:\n",
    "    # print(type(X_train[col]))\n",
    "    \n",
    "    X_train3[col] = X_train3[col].astype(str)\n",
    "    X_valid3[col] = X_valid3[col].astype(str)\n",
    "    # test[col] = test[col].astype(str)\n",
    "\n",
    "# print(X_train3[categorical_columns].dtypes)\n",
    "\n",
    "#pool\n",
    "''' categorical ë‚´ì¥ encoding '''\n",
    "train_pool = Pool(data = X_train3, label = y_train3, cat_features = categorical_columns)\n",
    "val_pool = Pool(data = X_valid3, label = y_valid3, cat_features = categorical_columns)\n",
    "\n",
    "# classes = np.unique(y_train3)\n",
    "# weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train3)\n",
    "# class_weights = dict(zip(classes, weights))\n",
    "\n",
    "#fitting\n",
    "model3 = CatBoostClassifier(cat_features = categorical_columns, random_state = 42, silent=True, eval_metric='AUC')\n",
    "model3.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "y_pred_prob = model3.predict_proba(X_valid3)[:, 1]\n",
    "auc_score = roc_auc_score(y_valid3, y_pred_prob)\n",
    "print(f\"Validation AUC-ROC: {auc_score:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' model 3 '''\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# categorical ë³€ìˆ˜ ë¬¸ìì—´ ë³€í™˜\n",
    "for col in categorical_columns:\n",
    "    X_train3[col] = X_train3[col].astype(str)\n",
    "    X_valid3[col] = X_valid3[col].astype(str)\n",
    "\n",
    "# CatBoost Pool ìƒì„±\n",
    "train_pool = Pool(data=X_train3, label=y_train3, cat_features=categorical_columns)\n",
    "val_pool = Pool(data=X_valid3, label=y_valid3, cat_features=categorical_columns)\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©\n",
    "best_params = {\n",
    "    'n_estimators': 1600,\n",
    "    'learning_rate': 0.022269562365768235,\n",
    "    'depth': 6,\n",
    "    'subsample': 0.8570762892949296,\n",
    "    'l2_leaf_reg': 2.736528145798781,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'colsample_bylevel': 0.7609486820290322,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'early_stopping_rounds': 70,\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_state': 42,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model3 = CatBoostClassifier(cat_features=categorical_columns, **best_params)\n",
    "model3.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred_prob = model3.predict_proba(X_valid3)[:, 1]\n",
    "auc_score = roc_auc_score(y_valid3, y_pred_prob)\n",
    "print(f\"Validation AUC-ROC: {auc_score:.8f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    X_train3[col] = X_train3[col].astype(str)\n",
    "    X_valid3[col] = X_valid3[col].astype(str)\n",
    "\n",
    "#pool\n",
    "''' categorical ë‚´ì¥ encoding '''\n",
    "train_pool = Pool(data = X_train3, label = y_train3, cat_features = categorical_columns)\n",
    "val_pool = Pool(data = X_valid3, label = y_valid3, cat_features = categorical_columns)\n",
    "\n",
    "classes = np.unique(y_train3)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train3)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Objective í•¨ìˆ˜\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # íŠ¸ë¦¬ ê°œìˆ˜: í•™ìŠµ ì†ë„ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì„ ìœ„í•´ ì ë‹¹í•œ ë²”ìœ„\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 2000, step=200),\n",
    "\n",
    "        # í•™ìŠµë¥ : ì•ˆì •ì„±ê³¼ ì†ë„ì˜ ê· í˜•ì„ ìœ„í•´ ì‘ì€ ë²”ìœ„\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "\n",
    "        # íŠ¸ë¦¬ ê¹Šì´: ê°€ì¥ íš¨ê³¼ì ì¸ ê¹Šì´ ë²”ìœ„\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 10),\n",
    "\n",
    "        # ë¶€ë¶„ ìƒ˜í”Œë§ ë¹„ìœ¨: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë°˜ì ì¸ ë²”ìœ„\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 0.9),\n",
    "\n",
    "        # ì •ê·œí™”: ì ì€ ê°’ê³¼ í° ê°’ ëª¨ë‘ íƒìƒ‰ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 2, 10),\n",
    "\n",
    "        # ë¦¬í”„ë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜: ì ë‹¹í•œ ë²”ìœ„ë¡œ ì„¤ì •\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 50, step=5),\n",
    "\n",
    "        # íŠ¸ë¦¬ ìˆ˜ì¤€ë³„ ìƒ˜í”Œë§ ë¹„ìœ¨: ì¼ë°˜ì ì¸ ë²”ìœ„\n",
    "        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.7, 1.0),\n",
    "\n",
    "        # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ë²•: ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì˜µì…˜ë§Œ ì œê³µ\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "\n",
    "        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "        \"class_weights\": class_weights,\n",
    "\n",
    "        # ë²”ì£¼í˜• ë³€ìˆ˜ ì§€ì •\n",
    "        \"cat_features\": categorical_columns,\n",
    "\n",
    "        # í‰ê°€ ì§€í‘œ\n",
    "        \"eval_metric\": \"AUC\",\n",
    "\n",
    "        # ì¡°ê¸° ì¤‘ë‹¨\n",
    "        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 30, 70, step=10),\n",
    "\n",
    "        # ë¡œê·¸ ìµœì†Œí™”\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    # Bayesian ë°©ì‹ì—ì„œëŠ” subsample ì œê±°\n",
    "    if params[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        del params[\"subsample\"]\n",
    "\n",
    "    cat_model = CatBoostClassifier(**params)\n",
    "    cat_model.fit(train_pool, eval_set=val_pool)\n",
    "    val_pred = cat_model.predict_proba(X_valid3)[:, 1]\n",
    "\n",
    "    return roc_auc_score(y_valid3, val_pred)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X_train_newf[col] = X_train_newf[col].astype(str)\n",
    "\n",
    "\n",
    "# KFold ì„¤ì •\n",
    "N_SPLITS = 5  # 5-Fold Cross Validation\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "classes = np.unique(y_train3)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train3)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Optuna Objective í•¨ìˆ˜ ì •ì˜\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # íŠ¸ë¦¬ ê°œìˆ˜: í•™ìŠµ ì†ë„ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì„ ìœ„í•´ ì ë‹¹í•œ ë²”ìœ„\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 2000, step=200),\n",
    "\n",
    "        # í•™ìŠµë¥ : ì•ˆì •ì„±ê³¼ ì†ë„ì˜ ê· í˜•ì„ ìœ„í•´ ì‘ì€ ë²”ìœ„\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "\n",
    "        # íŠ¸ë¦¬ ê¹Šì´: ê°€ì¥ íš¨ê³¼ì ì¸ ê¹Šì´ ë²”ìœ„\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 10),\n",
    "\n",
    "        # ë¶€ë¶„ ìƒ˜í”Œë§ ë¹„ìœ¨: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë°˜ì ì¸ ë²”ìœ„\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 0.9),\n",
    "\n",
    "        # ì •ê·œí™”: ì ì€ ê°’ê³¼ í° ê°’ ëª¨ë‘ íƒìƒ‰ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 2, 10),\n",
    "\n",
    "        # ë¦¬í”„ë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜: ì ë‹¹í•œ ë²”ìœ„ë¡œ ì„¤ì •\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 50, step=5),\n",
    "\n",
    "        # íŠ¸ë¦¬ ìˆ˜ì¤€ë³„ ìƒ˜í”Œë§ ë¹„ìœ¨: ì¼ë°˜ì ì¸ ë²”ìœ„\n",
    "        \"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.7, 1.0),\n",
    "\n",
    "        # ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ë²•: ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì˜µì…˜ë§Œ ì œê³µ\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "\n",
    "        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "        \"class_weights\": class_weights,\n",
    "\n",
    "        # ë²”ì£¼í˜• ë³€ìˆ˜ ì§€ì •\n",
    "        \"cat_features\": categorical_columns,\n",
    "\n",
    "        # í‰ê°€ ì§€í‘œ\n",
    "        \"eval_metric\": \"AUC\",\n",
    "\n",
    "        # ì¡°ê¸° ì¤‘ë‹¨\n",
    "        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 30, 70, step=10),\n",
    "\n",
    "        # ë¡œê·¸ ìµœì†Œí™”\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    # Bayesian ë°©ì‹ì—ì„œëŠ” subsample ì œê±°\n",
    "    if params[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        del params[\"subsample\"]\n",
    "\n",
    "    # K-Fold Cross Validation ìˆ˜í–‰\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train_newf, y_train3):\n",
    "        X_tr, X_val = X_train_newf.iloc[train_idx], X_train_newf.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train3.iloc[train_idx], y_train3.iloc[valid_idx]\n",
    "\n",
    "        train_pool = Pool(data=X_tr, label=y_tr, cat_features=categorical_columns)\n",
    "        val_pool = Pool(data=X_val, label=y_val, cat_features=categorical_columns)\n",
    "\n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        cat_model = CatBoostClassifier(**params)\n",
    "        cat_model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "        # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡\n",
    "        val_pred = cat_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # AUC ì ìˆ˜ ì €ì¥\n",
    "        auc_scores.append(roc_auc_score(y_val, val_pred))\n",
    "\n",
    "    # í‰ê·  AUC ë°˜í™˜\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Optuna Study ìƒì„± ë° ìµœì í™” ì‹¤í–‰\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' model 3 '''\n",
    "#categorical str transform\n",
    "for col in categorical_columns:\n",
    "    X_train_newf[col] = X_train_newf[col].astype(str)\n",
    "    test_newf[col] = test_newf[col].astype(str)\n",
    "\n",
    "#pool\n",
    "''' categorical ë‚´ì¥ encoding '''\n",
    "train_pool = Pool(data = X_train_newf, label = y, cat_features = categorical_columns)\n",
    "\n",
    "# classes = np.unique(y)\n",
    "# weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "# class_weights = dict(zip(classes, weights))\n",
    "\n",
    "#fitting\n",
    "model = CatBoostClassifier(cat_features = categorical_columns, random_state = 42, silent=True, eval_metric='AUC')\n",
    "model.fit(train_pool)\n",
    "\n",
    "pred_proba = model.predict_proba(test_newf)[:, 1]\n",
    "\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['probability'] = pred_proba\n",
    "\n",
    "submission.to_csv('./preprocessing_v4-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' model 3 '''\n",
    "# categorical ë³€ìˆ˜ ë¬¸ìì—´ ë³€í™˜\n",
    "for col in categorical_columns:\n",
    "    X_train_newf[col] = X_train_newf[col].astype(str)\n",
    "    test_newf[col] = test_newf[col].astype(str)\n",
    "\n",
    "# CatBoost Pool ìƒì„±\n",
    "train_pool = Pool(data=X_train_newf, label=y, cat_features=categorical_columns)\n",
    "\n",
    "# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©\n",
    "# best_params = {\n",
    "#     'n_estimators': 800,\n",
    "#     'learning_rate': 0.041481707162322246,\n",
    "#     'depth': 6,\n",
    "#     'l2_leaf_reg': 5.192930349370407,\n",
    "#     'min_data_in_leaf': 10,\n",
    "#     'colsample_bylevel': 0.8910668655874581,\n",
    "#     'bootstrap_type': 'Bayesian',\n",
    "#     'early_stopping_rounds': 60,\n",
    "#     'eval_metric': 'AUC',\n",
    "#     'random_state': 42,\n",
    "#     'verbose': 0\n",
    "# }\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = CatBoostClassifier(cat_features=categorical_columns,l2_leaf_reg=5)\n",
    "model.fit(train_pool)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "pred_proba = model.predict_proba(test_newf)[:, 1]\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['probability'] = pred_proba\n",
    "\n",
    "submission.to_csv('./preprocessing_v9.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivfcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
